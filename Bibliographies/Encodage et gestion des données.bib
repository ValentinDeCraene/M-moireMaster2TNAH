
@inproceedings{nagaiPersonalNameExtraction2015,
	title = {Personal Name Extraction from Japanese Historical Documents Using Machine Learning},
	doi = {10.1109/Culture.and.Computing.2015.46},
	abstract = {In this poster, we propose a method for extracting persons' real names and aliases from Japanese historical documents. In this method, we extract personal names and aliases by applying a named entity extraction technique based on machine learning using characters as the unit of analysis. One of the features of this method is that it uses already attached annotations to named entities in order to find undiscovered ones. Experimental results showed that our proposed method was able to extract personal names and aliases from "Yakusha-Hyoban-Ki", a collection of review documents of Kabuki actors in Edo Era (1603-1868) in Japan, with approximately 0.91 in F-measure.},
	pages = {207--208},
	booktitle = {International Conference on Culture and Computing},
	author = {Nagai, Noriyoshi and Kimura, Fuminori and Maeda, Akira and Akama, Ryo},
	date = {2015-10-01},
}

@article{groverNamedEntityRecognition2008,
	title = {Named Entity Recognition for Digitised Historical Texts},
	url = {https://www.research.ed.ac.uk/en/publications/named-entity-recognition-for-digitised-historical-texts},
	pages = {1343--1346},
	journaltitle = {Proceedings of the International Conference on Language Resources and Evaluation, {LREC} 2008, 26 May - 1 June 2008, Marrakech, Morocco},
	author = {Grover, Claire and Givon, Sharon and Tobin, Richard and Ball, Julian},
	urldate = {2022-05-02},
	date = {2008},
	file = {Full Text PDF:/home/valentin/snap/zotero-snap/common/Zotero/storage/QS4U524U/Grover et al. - 2008 - Named Entity Recognition for Digitised Historical .pdf:application/pdf;Snapshot:/home/valentin/snap/zotero-snap/common/Zotero/storage/KCXMGYYG/named-entity-recognition-for-digitised-historical-texts.html:text/html},
}

@thesis{fournerEtudeStructurationAutomatique2019,
	location = {Paris},
	title = {Étude de la structuration automatique et de l’éditorialisaion d’un corpus hétérogène, l’exemple des sources du conseil des prud’hommes pour le textile du {XIXe} siècle.},
	type = {Mémoire de master {TNAH}},
	author = {Fourner, Victoria Le},
	date = {2019},
	langid = {french},
	file = {Fourner - ÉCOLE NATIONALE DES CHARTES.pdf:/home/valentin/snap/zotero-snap/common/Zotero/storage/2BGAY8T6/Fourner - ÉCOLE NATIONALE DES CHARTES.pdf:application/pdf},
}

@book{bissonEditerInventaireAncien2019,
	title = {Editer un inventaire ancien en {XML}-{TEI} P5},
	url = {hal-02457987},
	author = {Bisson, Marie and Kuhry, Emmanuelle and Goloubkoff, Anne},
	date = {2019},
}

@thesis{corbieresCatalogueAuFichier2020,
	location = {Paris},
	title = {Du catalogue au fichier {TEI}, création d'un workflow pour encoder automatiquement en {XML}-{TEI} des catalogues d'exposition, mémoire de master {TNAH}},
	institution = {Ecole Nationale des Chartes},
	type = {phdthesis},
	author = {Corbières, Caroline},
	date = {2020},
}

@thesis{rondeaudunoyerEncoderAutomatiquementCatalogues2019,
	location = {Paris},
	title = {Encoder automatiquement des catalogues en {XML}/{TEI}, principes, évaluations et application à la revue des autographes de la librairie Charavay},
	institution = {Ecole Nationale des Chartes},
	type = {Mémoire de master {TNAH}},
	author = {Rondeau Du Noyer, Lucie},
	date = {2019},
}

@article{mangeotDictionnairesEditoriauxAux2013,
	title = {Des dictionnaires éditoriaux aux représentations {XML} standardisées},
	url = {https://hal.archives-ouvertes.fr/hal-00959229},
	doi = {10.1075/lis.30.08man},
	abstract = {Create an electronic dictionary from scratch is an expensive job because this task mobilizes over a long period, the work of skilled contributors, if not in lexicology, at least in linguistics. The use of specialized computer tools is essential for resources used by programs in natural language processing. When the socio-economic environment does not gather the necessary resources to the drafting of an electronic dictionary and printed dictionaries exist, these dictionaries are an important resource that can be used to initialize the creation of electronic lexical resources. This paper presents theoretical and practical aspects concerning the conversion of publishing dictionaries to electronic lexical resources. It takes into account the issue of limited economic resources, technology and the availability of qualified persons. Our field experiments concerns under-resourced languages mainly in Southeast Asia (Khmer, Malay, Vietnamese) and the Sahel (Bambara, Hausa, Kanuri, Tamajaq, Zarma), as most of the examples and socio-linguistic situations described in the paper relate to these areas. After a brief history devoted to the formats of electronic dictionaries ({SGML}, {XML}, {XSLT} and {CSS}), we present two standards that are dedicated to them (Text Encoding Initiative and Lexical Markup Framework). The issue of under-resourced languages is exposed and is followed by some examples concerning published dictionaries. The main technical challenges are detailed like the lack of standardization of the alphabets used and special characters (outside the traditional latin range). The conversion methodology is outlined and then detailed. The conversion to a bridge format in {XML} can be done by regular expressions or using specialized tools. Then, the bridge format is converted into the target format in {LMF}. The last part is dedicated to the consultation of resources through an online platform resource management.},
	pages = {24},
	journaltitle = {Nuria Gala et  Michael Zock. Ressources Lexicales : contenu, construction, utilisation, évaluation},
	author = {Mangeot, Mathieu and Enguehard, Chantal},
	urldate = {2022-04-12},
	date = {2013},
	langid = {french},
	note = {Publisher: John Benjamins},
	file = {Full Text PDF:/home/valentin/snap/zotero-snap/common/Zotero/storage/WNRED6D3/Mangeot et Enguehard - 2013 - Des dictionnaires éditoriaux aux représentations X.pdf:application/pdf;Snapshot:/home/valentin/snap/zotero-snap/common/Zotero/storage/J5W7LJXR/hal-00959229v2.html:text/html},
}

@article{ideCodageTEIDictionnaires1996,
	title = {Codage {TEI} des dictionnaires électroniques},
	doi = {10.5802/cg.197},
	journaltitle = {Cahiers {GUTenberg}},
	shortjournal = {Cahiers {GUTenberg}},
	author = {Ide, Nancy and Et, Ab and Véronis, Jean},
	date = {1996-01-01},
	file = {Full Text PDF:/home/valentin/snap/zotero-snap/common/Zotero/storage/QWS3J59F/Ide et al. - 1996 - Codage TEI des dictionnaires électroniques.pdf:application/pdf},
}

@inproceedings{khemakhemInformationExtractionWorkflow2020,
	location = {Zagreb / Virtual, Croatia},
	title = {Information Extraction Workflow for Digitised Entry-based Documents},
	url = {https://hal.archives-ouvertes.fr/hal-02508549},
	booktitle = {{DARIAH} Annual event 2020},
	author = {Khemakhem, Mohamed and Gabay, Simon and Joyeux-Prunel, Béatrice and Romary, Laurent and Saint-Raymond, Léa and Rondeau Du Noyer, Lucie},
	urldate = {2022-04-12},
	date = {2020-05},
	keywords = {Auction catalogue, Catalogue d'Exposition, Catalogue de vente aux enchères, Dictionary, Dictionnaire, Exhibition catalogue, {GROBID}-Dictionaries, {TEI}-{XML}},
	file = {HAL PDF Full Text:/home/valentin/snap/zotero-snap/common/Zotero/storage/J42QPLC3/Khemakhem et al. - 2020 - Information Extraction Workflow for Digitised Entr.pdf:application/pdf},
}

@inproceedings{khemakhemAutomaticExtractionTEI2017,
	location = {Leiden, Netherlands},
	title = {Automatic Extraction of {TEI} Structures in Digitized Lexical Resources using Conditional Random Fields},
	url = {https://hal.archives-ouvertes.fr/hal-01508868},
	abstract = {An important number of digitized lexical resources remain unexploited due to their unstructured content. Manually structuring such resources is a costly task given their multifold complexity. Our goal is to find an approach to automatically structure digitized dictionaries, independently from the language or the lexicographic school or style. In this paper we present a first version of {GROBID}-Dictionaries1, an open source machine learning system for lexical information extraction.
Our approach is twofold: we perform a cascading structure extraction, while we select at each level specific features for training.
We followed a ”divide to conquer” strategy to dismantle text constructs in a digitized dictionary, based on the observation of their layout. Main pages (see Figure 1) in almost any dictionary share three blocks: a header (green), a footer (blue) and a body (orange). The body is, in its turn, constituted by several entries (red). Each lexical entry can be further decomposed (see Figure 2) as: form (green), etymology (blue), sense (red) or/and related entry. The same logic could be applied further for each extracted block but in the scope of this paper we focus just on the first three levels.
The cascading approach ensures a better understanding of the learning process’s output and consequently simplifies the feature selection process. Limited exclusive text blocks per level helps significantly in diagnosing the cause of prediction errors. It allows an early detection and replacement of irrelevant selected features that can bias a trained model. In such a segmentation, it becomes more straightforward to notice that, for instance, the token position in the page is very relevant to detect headers and footers and has almost no pertinence for capturing a sense in a lexical entry which is very often split on two pages.
To implement our approach, we took up the available infrastructure from {GROBID} [7], a machine learning system for the extraction of bibliographic metadata. {GROBID} adopts the same cascading approach and uses Conditional Random Fields ({CRF}) [6] to label text sequences. The output of Grobid dictionary is planned to generate a {TEI} compliant encoding [2, 9] where the various segmentation levels are associated with an appropriate {XML} tessellation. Collaboration with {COST} {ENeL} are ongoing to ensure maximal compatibility with existing dictionary projects.
Our experiments justify so far our choices, where models for the first two levels trained on two different dictionary samples have given a high precision and recall with a small amount of annotated data. Relying mainly on the text layout, we tried to diversify the selected features for each model, on the token and line levels. We are working on tuning features and annotating more data to maintain the good results with new samples and to improve the third segmentation level.
While just few task specific attempts [1] have been using machine learning in this research direction, the landscape remains dominated by rule based techniquess [4, 3, 8] which are ad-hoc and costly, even impossible, to adapt for new lexical resources.},
	booktitle = {electronic lexicography, {eLex} 2017},
	author = {Khemakhem, Mohamed and Foppiano, Luca and Romary, Laurent},
	urldate = {2022-04-12},
	date = {2017-09},
	keywords = {automatic structuring, {CRF}, digitized dictionaries, machine learning, {TEI}},
	file = {HAL PDF Full Text:/home/valentin/snap/zotero-snap/common/Zotero/storage/IEXHMQEA/Khemakhem et al. - 2017 - Automatic Extraction of TEI Structures in Digitize.pdf:application/pdf},
}

@inproceedings{bohbotPresentingNenufarProject2018,
	location = {Miyazaki, Japan},
	title = {Presenting the Nénufar Project: a Diachronic Digital Edition of the Petit Larousse Illustré},
	url = {https://hal.archives-ouvertes.fr/hal-01728328},
	shorttitle = {Presenting the Nénufar Project},
	abstract = {This paper presents the Nénufar project, which aims to make several successive (free of copyright up to 1948) editions of the French Petit Larousse Illustré dictionary available in a digitised format. The corpus of digital editions will be made publicly available via a web-based querying interface, as well as distributed in a machine readable format, {TEI}-{LEX}0.},
	pages = {1--6},
	booktitle = {{GLOBALEX} 2018 - Globalex workshop at {LREC}2018},
	author = {Bohbot, Hervé and Frontini, Francesca and Luxardo, Giancarlo and Khemakhem, Mohamed and Romary, Laurent},
	urldate = {2022-04-12},
	date = {2018-05},
	keywords = {Dictionaries, Petit Larousse, {TEI}},
	file = {HAL PDF Full Text:/home/valentin/snap/zotero-snap/common/Zotero/storage/845KDW8G/Bohbot et al. - 2018 - Presenting the Nénufar Project a Diachronic Digit.pdf:application/pdf},
}

@article{morthModelingFrequencyData2014,
	title = {Modeling Frequency Data: Methodological Considerations on the Relationship between Dictionaries and Corpora},
	rights = {For this publication a Creative Commons Attribution 4.0 International license has been granted by the author(s) who retain full copyright.},
	issn = {2162-5603},
	url = {https://journals.openedition.org/jtei/1356},
	doi = {10.4000/jtei.1356},
	shorttitle = {Modeling Frequency Data},
	abstract = {Academic dictionary writing is making greater and greater use of the {TEI} Guidelines’ dictionary module. And as increasing numbers of {TEI} dictionaries become available, there is an ever more palpable need to work towards greater interoperability among dictionary writing systems and other language resources that are needed by dictionaries and dictionary tools. In particular this holds true for the crucial role that statistical data obtained from language resources play in lexicographic workflow—a role that also has to be reflected in the model of the data produced in these workflows. Presenting a range of current projects, the authors address two main questions in this area: How can the relationship between a dictionary and other language resources be conceptualized, irrespective of whether they are used in the production of the dictionary or to enrich existing lexicographic data? And how can this be documented using the {TEI} Guidelines? Discussing a variety of options, this paper proposes a customization of the {TEI} dictionary module that tries to respond to the emerging requirements in an environment of increasingly intertwined language resources.},
	issue = {Issue 8},
	journaltitle = {Journal of the Text Encoding Initiative},
	author = {Mörth, Karlheinz and Romary, Laurent and Budin, Gerhard and Schopper, Daniel},
	urldate = {2022-04-12},
	date = {2014-12-28},
	langid = {english},
	note = {Number: Issue 8
Publisher: Text Encoding Initiative Consortium},
	keywords = {A ficher, digital corpora, language resources, lexicography, statistics},
	file = {Full Text PDF:/home/valentin/snap/zotero-snap/common/Zotero/storage/7G6TP86Y/Mörth et al. - 2014 - Modeling Frequency Data Methodological Considerat.pdf:application/pdf},
}

@article{budinCreatingLexicalResources2012,
	title = {Creating Lexical Resources in {TEI} P5},
	rights = {{TEI} Consortium 2012 (Creative Commons Attribution-{NoDerivs} 3.0 Unported License )},
	issn = {2162-5603},
	url = {https://journals.openedition.org/jtei/522},
	doi = {10.4000/jtei.522},
	abstract = {Although most of the relevant dictionary productions of the recent past have relied on digital data and methods, there is little consensus on formats and standards. The Institute for Corpus Linguistics and Text Technology ({ICLTT}) of the Austrian Academy of Sciences has been conducting a number of varied lexicographic projects, both digitising print dictionaries and working on the creation of genuinely digital lexicographic data. This data was designed to serve varying purposes: machine-readability was only one. A second goal was interoperability with digital {NLP} tools. To achieve this end, a uniform encoding system applicable across all the projects was developed. The paper describes the constraints imposed on the content models of the various elements of the {TEI} dictionary module and provides arguments in favour of {TEI} P5 as an encoding system not only being used to represent digitised print dictionaries but also for {NLP} purposes.},
	issue = {Issue 3},
	journaltitle = {Journal of the Text Encoding Initiative},
	author = {Budin, Gerhard and Majewski, Stefan and Mörth, Karlheinz},
	urldate = {2022-04-12},
	date = {2012-11-05},
	langid = {english},
	note = {Number: Issue 3
Publisher: Text Encoding Initiative Consortium},
	keywords = {dictionaries, digital lexicography, {NLP}, P5},
	file = {Full Text PDF:/home/valentin/snap/zotero-snap/common/Zotero/storage/YJXXN7MD/Budin et al. - 2012 - Creating Lexical Resources in TEI P5.pdf:application/pdf},
}

@mvbook{teiconsortiumTEIP5Guidelines2008a,
	location = {Oxford, Royaume-Uni de Grande-Bretagne et d'Irlande du Nord},
	title = {{TEI} P5: guidelines for Electronic Text Encoding and Interchange},
	isbn = {978-0-9821226-0-0},
	shorttitle = {{TEI} P5},
	volumes = {2},
	pagetotal = {1307},
	author = {{TEI} Consortium},
	editor = {Bauman, Syd and Burnard, Lou},
	date = {2008},
	keywords = {Publications électroniques, Structures de données (informatique), {XML} (langage de balisage)},
	file = {Library Catalog Entry Snapshot:/home/valentin/snap/zotero-snap/common/Zotero/storage/UBKLKNWB/SRCH.html:text/html},
}

@article{schwartzModelingBornDigitalFactoid2022,
	title = {Modeling a Born-Digital Factoid Prosopography using the {TEI} and Linked Data},
	rights = {For this publication a Creative Commons Attribution 4.0 International license has been granted by the author(s) who retain full copyright.},
	issn = {2162-5603},
	url = {https://journals.openedition.org/jtei/3979},
	doi = {10.4000/jtei.3979},
	abstract = {Although the {TEI} has traditionally been used for encoding text, its combination of structured and semi-structured data has made it a compelling choice for born-digital, linked-data resources as well. Our intent here is to demonstrate the advantages it offers for digital prosopographies along with a model that can be used for them. Syriac Persons, Events, and Relations ({SPEAR}) is a born-digital prosopography project in the field of Syriac studies. Where traditional prosopographies focused on prose descriptions of individual persons of significance, {SPEAR} follows recent developments in research methodologies that instead produce prosopographical factoids. Factoids are structured data about persons drawn from the analysis of historical texts. Most factoid prosopographies use relational databases to model data. Instead, {SPEAR} uses a customized {TEI} schema to model factoids that can be queried and visualized in an {XML} database as well as serialized in {HTML} for human viewers and in {RDF} for data sharing. The {TEI}’s provisions for structured and semi-structured data make it ideal for encoding data from heterogeneous historical source material. Moreover, its linking capabilities connect {SPEAR} data to related data sets. By modeling prosopographical factoids, and not the source texts themselves, {SPEAR} offers an example of how a born-digital, data-oriented approach to using the {TEI} can circumvent some of the challenges posed by the tree structure of {XML}. It also disrupts traditional understandings of data and stand-off markup through combining linked open data approaches with the use of the {TEI}.},
	journaltitle = {Journal of the Text Encoding Initiative},
	author = {Schwartz, Daniel L. and Gibson, Nathan P. and Torabi, Katayoun},
	urldate = {2022-06-21},
	date = {2022-03-21},
	langid = {english},
	note = {Publisher: Text Encoding Initiative Consortium},
	keywords = {A ficher, factoids, Linked Open Data, prosopography, stand-off markup, Syriac studies},
	file = {Full Text PDF:/home/valentin/snap/zotero-snap/common/Zotero/storage/2LXLDNHM/Schwartz et al. - 2022 - Modeling a Born-Digital Factoid Prosopography usin.pdf:application/pdf},
}

@online{burnardODDChainingBeginners,
	title = {{ODD} Chaining for Beginners},
	url = {http://teic.github.io/TCW/howtoChain.html},
	author = {Burnard, Loui},
	urldate = {2022-06-28},
	file = {ODD Chaining for Beginners:/home/valentin/snap/zotero-snap/common/Zotero/storage/F92G9JAE/howtoChain.html:text/html},
}

@thesis{janesCataloguePapierAu2021,
	location = {Paris},
	title = {Du catalogue papier au numérique Une chaîne de traitement ouverte pour l’extraction d’informations issues de documents structurés},
	url = {https://raw.githubusercontent.com/Juliettejns/Memoire_TNAH/main/Jjanes_Memoire.pdf},
	institution = {Ecole nationale des chartes},
	type = {Mémoire de master "Technologies numériques appliquées à l'histoire"},
	author = {Janès, Juliette},
	urldate = {2022-05-29},
	date = {2021},
}

@article{zunkeJSONVsXML2014,
	title = {{JSON} vs {XML}: A Comparative Performance Analysis of Data Exchange Formats},
	volume = {3},
	abstract = {Service Oriented Architecture is integrated in the very fabric of the nature of Internet. The Internet as it exists today is made up of numerous components which operate asynchronously. These independent components communicate with each other using a specific set of formats that standardize the communication and regulate the coherence of the communication. The {SOA} structure is made up of multiple loosely coupled components that can be broadly classified as service producers and service consumers. The loose coupling allows enterprises to respond to changes quickly by updating, replacing and changing concerned modules with flexibility, without affecting other components coupled to it. This paper first compares the features of the two most widely used Data Exchange formats for communication between these components and finally, provides a comparative analysis on the performance of these formats using benchmarks.},
	pages = {5},
	number = {4},
	journaltitle = {International Journal of Computer Science and Network,},
	author = {Zunke, Saurabh and D’Souza, Veronica},
	date = {2014},
	langid = {english},
	file = {Zunke et D’Souza - 2014 - JSON vs XML A Comparative Performance Analysis of.pdf:/home/valentin/snap/zotero-snap/common/Zotero/storage/W496IUXJ/Zunke et D’Souza - 2014 - JSON vs XML A Comparative Performance Analysis of.pdf:application/pdf},
}

@book{burnardQuEstceQue2015a,
	location = {Marseille, France},
	title = {Qu’est-ce que la Text Encoding Initiative ?},
	isbn = {978-2-8218-5581-6},
	url = {http://books.openedition.org/oep/1237},
	abstract = {Les Guidelines de la Text Encoding Initiative ({TEI}) sont depuis longtemps considérées comme le standard de fait pour la préparation de ressources textuelles numériques dans la communauté académique. Elles offrent au débutant un éventail de possibilités qui peut paraître intimidant, mais qui reflète l’impressionnante étendue des applications possibles pour l’encodage de texte, depuis les éditions critiques traditionnelles jusqu’aux corpus linguistiques, aux lexiques historiques, aux archives numériques et au-delà.  Utilisant de nombreux exemples de textes encodés en {TEI} issus de domaines variés, ce livre simple et direct est destiné à aider le débutant à faire ses propres choix parmi les multiples options offertes par la {TEI}. Il explique la technologie {XML} utilisée par la {TEI} d’une manière accessible au lecteur dépourvu de formation technique, et offre une visite guidée des dédales de l’univers de la {TEI}, et de la façon dont elle peut être personnalisée pour répondre aux besoins d’un projet particulier.  Cet ouvrage a été réalisé avec le soutien de la région Provences-Alpes-Côte d’Azur},
	publisher = {{OpenEdition} Press},
	author = {Burnard, Lou},
	urldate = {2022-08-15},
	date = {2015},
	keywords = {Informatique, Langages de balisage},
	file = {Library Catalog Entry Snapshot:/home/valentin/snap/zotero-snap/common/Zotero/storage/EPWAA2QA/SRCH.html:text/html},
}